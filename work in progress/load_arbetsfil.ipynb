{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/jon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/jon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## NLTK SET UP ##\n",
    "\n",
    "## Natural language processing, pandas setup\n",
    "\n",
    "#Import tokenization, download stopwords & punkt. Download not needed each run but left in for compatibility.\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import download \n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "## Feed variable with NLTK stopwords\n",
    "stop_words = set(stopwords.words(\"swedish\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/srb9_l2x3gncdpn0_zx0r2pm0000gn/T/ipykernel_5397/3695825639.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['cleaned_occupation'] = cleaned_occupations\n",
      "/var/folders/09/srb9_l2x3gncdpn0_zx0r2pm0000gn/T/ipykernel_5397/3695825639.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['cleaned_description'] = cleaned_descriptions\n",
      "/var/folders/09/srb9_l2x3gncdpn0_zx0r2pm0000gn/T/ipykernel_5397/3695825639.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['cleaned_description'] = df['cleaned_description'].str.translate(str.maketrans(replace_dict))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 occupation                                        description\n",
      "0            Projektledare,  {'text': 'Tänk dig de mest högteknologiska ele...\n",
      "1    Databasadministratör',  {'text': 'I denna projektanställning på deltid...\n",
      "2     Journalist/Reporter',  {'text': 'Deltidsskribenter med stort nyhets- ...\n",
      "3     Handläggare/Utredare,  {'text': 'Är du social och vet vad god service...\n",
      "4         Transportledare',  {'text': 'Vill du arbeta med framtidens skogst...\n",
      "..                      ...                                                ...\n",
      "882      Trafiksamordnare',  {'text': 'Göteborg är mitt uppe i ett stort st...\n",
      "883         Svetsingenjör',  {'text': 'För kunds räkning söker vi en Svetsa...\n",
      "884           Komminister',  {'text': 'Beskrivning\\nVi söker dig som vill h...\n",
      "885              Inspektör,  {'text': 'Är du redo för en större uppgift?\\nA...\n",
      "886    Förvaltningsekonom',  {'text': 'Du blir en del av ett team med tätt ...\n",
      "\n",
      "[887 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# read the file and select the relevant columns\n",
    "jobtech_dataset = pd.read_csv('dfsvenska.csv')\n",
    "df = jobtech_dataset[['description', 'occupation']]\n",
    "\n",
    "replace_dict = {'!': '', '@': '', '#': '', '$': '', '%': '', '\\n': ''}\n",
    "\n",
    "# clean the occupation column and create a new column for the cleaned occupation\n",
    "cleaned_occupations = []\n",
    "for index, row in df.iterrows():\n",
    "    occupation = row['occupation'][41:]\n",
    "    cleaned_occupation = occupation.split()[0]\n",
    "    cleaned_occupations.append(cleaned_occupation)\n",
    "df['cleaned_occupation'] = cleaned_occupations\n",
    "\n",
    "cleaned_descriptions = []\n",
    "for index, row in df.iterrows():\n",
    "    description = row['description'][10:]\n",
    "    cleaned_descriptions.append(description)\n",
    "df['cleaned_description'] = cleaned_descriptions\n",
    "\n",
    "\n",
    "df['cleaned_description'] = df['cleaned_description'].str.translate(str.maketrans(replace_dict))\n",
    "\n",
    "\n",
    "# loop through each unique occupation and find the corresponding description(s)\n",
    "occupations = df['cleaned_occupation'].unique()\n",
    "occupation_descriptions = []\n",
    "for occupation in occupations:\n",
    "    description = \"\"\n",
    "    for index, row in df.iterrows():\n",
    "        if row['cleaned_occupation'] == occupation:\n",
    "            description += row['description'] + \" \"\n",
    "    occupation_descriptions.append(description.strip())\n",
    "\n",
    "# create a new dataframe with the occupations and descriptions\n",
    "new_df = pd.DataFrame({'occupation': occupations, 'description': occupation_descriptions})\n",
    "\n",
    "new_df.to_csv('clean_ness.csv', index=False)\n",
    "\n",
    "# print the new dataframe\n",
    "print(new_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "\n",
    "# Code from Canvas:\n",
    "#load pandas, tool for data analysis in Python\n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "# load nltk to detect stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "replace_dict = {'OM TJÄNSTEN': ' ', 'ARBETSUPPGIFTER': ' ', 'VI SÖKER DIG SOM': ' ', \n",
    "                 'INFORMATION OM FÖRETAGET': ' ', \"'\" : '', 'text_formatted' : '',\n",
    "                 'company_information' : '', 'needs' : '', 'requirements' : '', \n",
    "                 'conditions' : '', 'None' : '', '!': ' ', ':' : '', ',' : ''} \n",
    "\n",
    "\n",
    "\n",
    "# read the file called 2022.json that is in the same directory and call it jobtech_dataset\n",
    "jobtech_dataset = pd.read_csv('jobtech_dataset2022.csv')\n",
    "# #show the variables names (columns) in the dataset\n",
    "print(jobtech_dataset.columns)\n",
    "#show the first 3 rows (job postings) in the dataset\n",
    "print(jobtech_dataset.head(3))\n",
    "\n",
    "#jobtech_dataset.info()\n",
    "\n",
    "# crop dataset to only wanted columns\n",
    "dataframe = jobtech_dataset[['description', 'occupation']] \n",
    "\n",
    "#dataframe.info()\n",
    "\n",
    "df = pd.DataFrame(columns=dataframe.columns)\n",
    "\n",
    "print('detecting swedish ads..')\n",
    "\n",
    "# iterate over the rows of the original dataframe\n",
    "for index, row in dataframe.iterrows():\n",
    "    # detect the language of the description column using the langdetect library\n",
    "    if detect(row['description']) == 'sv':\n",
    "        # if the language is Swedish, append the row to the new dataframe\n",
    "        df = df.append(row, ignore_index=True)\n",
    "\n",
    "# cleaning the column description\n",
    "print('cleaning description column..')\n",
    "df['description'] = df['description'].str.slice(start=10)\n",
    "df['description'] = df['description'].str.replace('\\\\', 'KOWABUNGA')\n",
    "df['description'] = df['description'].str.replace('KOWABUNGAn', ' ')\n",
    "df['description'] = df['description'].str.replace('KOWABUNGAu202f', ' ')\n",
    "df['description'] = df['description'].str.replace('KOWABUNGAt', ' ')\n",
    "df['description'] = df['description'].str.replace('KOWABUNGAxad', ' ')\n",
    "df['description'] = df['description'].str.replace('KOWABUNGAx95', ' ')\n",
    "df['description'] = df['description'].str.replace('KOWABUNGAxa0', ' ')\n",
    "df['description'] = df['description'].str.replace('KOWABUNGA', ' ')\n",
    "df['description'] = df['description'].replace(replace_dict, regex=True)\n",
    "\n",
    "#Remove stop words from description column\n",
    "print('removing stopwords..')\n",
    "stop_words = set(stopwords.words('swedish'))\n",
    "df['description'] = df['description'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word.lower() not in stop_words]))\n",
    "\n",
    "\n",
    "# Clean Occupation column\n",
    "print('cleaning occupation column..')\n",
    "df['occupation'] = df['occupation'].str.slice(start=40)\n",
    "df['occupation'] = df['occupation'].str.split(\"'\", 2).str[1]\n",
    "\n",
    "'''\n",
    "print(df.columns)\n",
    "#show the first 3 rows (job postings) in the dataset\n",
    "print(df.head(3))\n",
    "df.info()'''\n",
    "\n",
    "# loop through each unique occupation and find the corresponding description(s)\n",
    "print('finding unique occupations..')\n",
    "occupations = df['occupation'].unique()\n",
    "occupation_descriptions = []\n",
    "for occupation in occupations:\n",
    "    description = \"\"\n",
    "    for index, row in df.iterrows():\n",
    "        if row['occupation'] == occupation:\n",
    "            description += row['description'] + \" \"\n",
    "    occupation_descriptions.append(description.strip())\n",
    "\n",
    "# create a new dataframe with the occupations and descriptions\n",
    "new_df = pd.DataFrame({'occupation': occupations, 'description': occupation_descriptions})\n",
    "\n",
    "# print the new dataframe\n",
    "print(new_df)\n",
    "\n",
    "# save the new dataframe to a CSV file\n",
    "new_df.to_csv('dataset2022.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'external_id', 'webpage_url', 'logo_url', 'headline',\n",
      "       'application_deadline', 'number_of_vacancies', 'salary_description',\n",
      "       'access', 'experience_required', 'access_to_own_car',\n",
      "       'application_contacts', 'publication_date', 'last_publication_date',\n",
      "       'removed', 'removed_date', 'source_type', 'timestamp',\n",
      "       'description.text', 'description.text_formatted',\n",
      "       'description.company_information', 'description.needs',\n",
      "       'description.requirements', 'description.conditions',\n",
      "       'employment_type.concept_id', 'employment_type.label',\n",
      "       'employment_type.legacy_ams_taxonomy_id', 'salary_type.concept_id',\n",
      "       'salary_type.label', 'salary_type.legacy_ams_taxonomy_id',\n",
      "       'duration.concept_id', 'duration.label',\n",
      "       'duration.legacy_ams_taxonomy_id', 'working_hours_type.concept_id',\n",
      "       'working_hours_type.label', 'working_hours_type.legacy_ams_taxonomy_id',\n",
      "       'scope_of_work.min', 'scope_of_work.max', 'employer.phone_number',\n",
      "       'employer.email', 'employer.url', 'employer.organization_number',\n",
      "       'employer.name', 'employer.workplace',\n",
      "       'application_details.information', 'application_details.reference',\n",
      "       'application_details.email', 'application_details.via_af',\n",
      "       'application_details.url', 'application_details.other',\n",
      "       'occupation.concept_id', 'occupation.label',\n",
      "       'occupation.legacy_ams_taxonomy_id', 'occupation_group.concept_id',\n",
      "       'occupation_group.label', 'occupation_group.legacy_ams_taxonomy_id',\n",
      "       'occupation_field.concept_id', 'occupation_field.label',\n",
      "       'occupation_field.legacy_ams_taxonomy_id',\n",
      "       'workplace_address.municipality', 'workplace_address.municipality_code',\n",
      "       'workplace_address.municipality_concept_id', 'workplace_address.region',\n",
      "       'workplace_address.region_code', 'workplace_address.region_concept_id',\n",
      "       'workplace_address.country', 'workplace_address.country_code',\n",
      "       'workplace_address.country_concept_id',\n",
      "       'workplace_address.street_address', 'workplace_address.postcode',\n",
      "       'workplace_address.city'],\n",
      "      dtype='object')\n",
      "         id              external_id  \\\n",
      "0  25478909  46-556559-5450-15063546   \n",
      "1  25416534  46-556559-5450-15064947   \n",
      "2  25437354  46-556559-5450-15065200   \n",
      "\n",
      "                                         webpage_url  \\\n",
      "0  https://arbetsformedlingen.se/platsbanken/anno...   \n",
      "1  https://arbetsformedlingen.se/platsbanken/anno...   \n",
      "2  https://arbetsformedlingen.se/platsbanken/anno...   \n",
      "\n",
      "                                            logo_url  \\\n",
      "0  https://www.arbetsformedlingen.se/rest/arbetsg...   \n",
      "1  https://www.arbetsformedlingen.se/rest/arbetsg...   \n",
      "2  https://www.arbetsformedlingen.se/rest/arbetsg...   \n",
      "\n",
      "                                        headline application_deadline  \\\n",
      "0                   Service Engineer to Mycronic  2022-01-31T23:59:59   \n",
      "1   Teknisk projektledare inom R&D till Mycronic  2022-01-31T23:59:59   \n",
      "2  Samordnande student till innovativa Immuneed!  2022-01-31T23:59:59   \n",
      "\n",
      "   number_of_vacancies salary_description access  experience_required  ...  \\\n",
      "0                  1.0       Enligt avtal    NaN                 True  ...   \n",
      "1                  1.0       Enligt avtal    NaN                 True  ...   \n",
      "2                  1.0       Enligt avtal    NaN                 True  ...   \n",
      "\n",
      "   workplace_address.municipality_concept_id  workplace_address.region  \\\n",
      "0                               AvNB_uwa_6n6            Stockholms län   \n",
      "1                               AvNB_uwa_6n6            Stockholms län   \n",
      "2                               otaF_bQY_4ZD               Uppsala län   \n",
      "\n",
      "  workplace_address.region_code workplace_address.region_concept_id  \\\n",
      "0                           1.0                        CifL_Rzy_Mku   \n",
      "1                           1.0                        CifL_Rzy_Mku   \n",
      "2                           3.0                        zBon_eET_fFU   \n",
      "\n",
      "   workplace_address.country  workplace_address.country_code  \\\n",
      "0                    Sverige                             199   \n",
      "1                    Sverige                             199   \n",
      "2                    Sverige                             199   \n",
      "\n",
      "  workplace_address.country_concept_id  workplace_address.street_address  \\\n",
      "0                         i46j_HmG_v64                               NaN   \n",
      "1                         i46j_HmG_v64                               NaN   \n",
      "2                         i46j_HmG_v64                               NaN   \n",
      "\n",
      "  workplace_address.postcode  workplace_address.city  \n",
      "0                        NaN                     NaN  \n",
      "1                        NaN                     NaN  \n",
      "2                        NaN                     NaN  \n",
      "\n",
      "[3 rows x 71 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 356695 entries, 0 to 356694\n",
      "Data columns (total 71 columns):\n",
      " #   Column                                     Non-Null Count   Dtype  \n",
      "---  ------                                     --------------   -----  \n",
      " 0   id                                         356695 non-null  int64  \n",
      " 1   external_id                                252222 non-null  object \n",
      " 2   webpage_url                                356695 non-null  object \n",
      " 3   logo_url                                   268231 non-null  object \n",
      " 4   headline                                   356695 non-null  object \n",
      " 5   application_deadline                       356695 non-null  object \n",
      " 6   number_of_vacancies                        356624 non-null  float64\n",
      " 7   salary_description                         235742 non-null  object \n",
      " 8   access                                     430 non-null     object \n",
      " 9   experience_required                        356695 non-null  bool   \n",
      " 10  access_to_own_car                          356695 non-null  bool   \n",
      " 11  application_contacts                       0 non-null       float64\n",
      " 12  publication_date                           356695 non-null  object \n",
      " 13  last_publication_date                      356695 non-null  object \n",
      " 14  removed                                    356695 non-null  bool   \n",
      " 15  removed_date                               0 non-null       float64\n",
      " 16  source_type                                356695 non-null  object \n",
      " 17  timestamp                                  356695 non-null  int64  \n",
      " 18  description.text                           356656 non-null  object \n",
      " 19  description.text_formatted                 0 non-null       float64\n",
      " 20  description.company_information            0 non-null       float64\n",
      " 21  description.needs                          0 non-null       float64\n",
      " 22  description.requirements                   0 non-null       float64\n",
      " 23  description.conditions                     240736 non-null  object \n",
      " 24  employment_type.concept_id                 356695 non-null  object \n",
      " 25  employment_type.label                      356695 non-null  object \n",
      " 26  employment_type.legacy_ams_taxonomy_id     356695 non-null  int64  \n",
      " 27  salary_type.concept_id                     356695 non-null  object \n",
      " 28  salary_type.label                          356695 non-null  object \n",
      " 29  salary_type.legacy_ams_taxonomy_id         356695 non-null  int64  \n",
      " 30  duration.concept_id                        341621 non-null  object \n",
      " 31  duration.label                             341621 non-null  object \n",
      " 32  duration.legacy_ams_taxonomy_id            341621 non-null  float64\n",
      " 33  working_hours_type.concept_id              341621 non-null  object \n",
      " 34  working_hours_type.label                   341621 non-null  object \n",
      " 35  working_hours_type.legacy_ams_taxonomy_id  341621 non-null  float64\n",
      " 36  scope_of_work.min                          356694 non-null  float64\n",
      " 37  scope_of_work.max                          341621 non-null  float64\n",
      " 38  employer.phone_number                      0 non-null       float64\n",
      " 39  employer.email                             0 non-null       float64\n",
      " 40  employer.url                               206751 non-null  object \n",
      " 41  employer.organization_number               0 non-null       float64\n",
      " 42  employer.name                              356672 non-null  object \n",
      " 43  employer.workplace                         356695 non-null  object \n",
      " 44  application_details.information            0 non-null       float64\n",
      " 45  application_details.reference              0 non-null       float64\n",
      " 46  application_details.email                  0 non-null       float64\n",
      " 47  application_details.via_af                 356695 non-null  bool   \n",
      " 48  application_details.url                    293725 non-null  object \n",
      " 49  application_details.other                  1214 non-null    object \n",
      " 50  occupation.concept_id                      356695 non-null  object \n",
      " 51  occupation.label                           356695 non-null  object \n",
      " 52  occupation.legacy_ams_taxonomy_id          356695 non-null  int64  \n",
      " 53  occupation_group.concept_id                356695 non-null  object \n",
      " 54  occupation_group.label                     356695 non-null  object \n",
      " 55  occupation_group.legacy_ams_taxonomy_id    356695 non-null  int64  \n",
      " 56  occupation_field.concept_id                356695 non-null  object \n",
      " 57  occupation_field.label                     356695 non-null  object \n",
      " 58  occupation_field.legacy_ams_taxonomy_id    356695 non-null  int64  \n",
      " 59  workplace_address.municipality             348768 non-null  object \n",
      " 60  workplace_address.municipality_code        348768 non-null  float64\n",
      " 61  workplace_address.municipality_concept_id  348768 non-null  object \n",
      " 62  workplace_address.region                   339760 non-null  object \n",
      " 63  workplace_address.region_code              339758 non-null  float64\n",
      " 64  workplace_address.region_concept_id        349097 non-null  object \n",
      " 65  workplace_address.country                  356695 non-null  object \n",
      " 66  workplace_address.country_code             356695 non-null  int64  \n",
      " 67  workplace_address.country_concept_id       356695 non-null  object \n",
      " 68  workplace_address.street_address           66827 non-null   object \n",
      " 69  workplace_address.postcode                 89299 non-null   float64\n",
      " 70  workplace_address.city                     89299 non-null   object \n",
      "dtypes: bool(4), float64(20), int64(8), object(39)\n",
      "memory usage: 183.7+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# read the file called 2022.json that is in the same directory and call it jobtech_dataset\n",
    "jobtech_dataset = pd.read_csv('/Users/jon/Documents/GitHub/PathFinder/jobtech_temp2022Rall_UPDATED.csv')\n",
    "# #show the variables names (columns) in the dataset\n",
    "print(jobtech_dataset.columns)\n",
    "#show the first 3 rows (job postings) in the dataset\n",
    "print(jobtech_dataset.head(3))\n",
    "\n",
    "# crop dataset to only wanted columns\n",
    "dataframe = jobtech_dataset[['description.text', 'occupation.label']] \n",
    "\n",
    "neww_df = dataframe\n",
    "jobtech_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read the file called 2022.json that is in the same directory and call it jobtech_dataset\n",
    "jobtech_dataset = pd.read_csv('jobtech_temp2022Rall_UPDATED.csv')\n",
    "\n",
    "# show the first 3 rows (job postings) in the dataset\n",
    "#first_three_rows = jobtech_dataset.head(3)\n",
    "# crop dataset to only wanted columns\n",
    "dataframe = jobtech_dataset[['description.text', 'occupation.label']] \n",
    "\n",
    "\n",
    "# save the first three rows as a new CSV file\n",
    "dataframe.to_csv('jobtech2022Rall2columns.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read the file called 2022.json that is in the same directory and call it jobtech_dataset\n",
    "jobtech_dataset = pd.read_csv('jobtech2022Rall2columns.csv')\n",
    "\n",
    "# rename columns\n",
    "jobtech_dataset = jobtech_dataset.rename(columns={'description.text': 'description', 'occupation.label': 'occupation'})\n",
    "\n",
    "# save the modified DataFrame to a new CSV file\n",
    "jobtech_dataset.to_csv('jobtech2022columns2updated.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read the file called 2022.json that is in the same directory and call it jobtech_dataset\n",
    "jobtech_dataset = pd.read_csv('jobtech2022columns2updated.csv')\n",
    "\n",
    "# show the first 3 rows (job postings) in the dataset\n",
    "dataframe = jobtech_dataset.head(100)\n",
    "# crop dataset to only wanted columns\n",
    "#dataframe = jobtech_dataset[['description.text', 'occupation.label']] \n",
    "\n",
    "\n",
    "# save the first three rows as a new CSV file\n",
    "dataframe.to_csv('jobtech2022Rall2columns100rows.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['description', 'occupation'], dtype='object')\n",
      "                                         description               occupation\n",
      "0  We are looking for you who want to go in and b...  Servicetekniker, maskin\n",
      "1  Tänk dig de mest högteknologiska elektronikpro...        Projektledare, IT\n",
      "2  I denna projektanställning på deltid kommer du...     Databasadministratör\n",
      "detecting swedish ads..\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/09/srb9_l2x3gncdpn0_zx0r2pm0000gn/T/ipykernel_740/2576393221.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'sv'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0;31m# Add the Swedish text to the data frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mlangdetect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang_detect_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLangDetectException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;31m# Handle the case when language detection fails\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[1;32m   8963\u001b[0m             \u001b[0mto_concat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8964\u001b[0m         return (\n\u001b[0;32m-> 8965\u001b[0;31m             concat(\n\u001b[0m\u001b[1;32m   8966\u001b[0m                 \u001b[0mto_concat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8967\u001b[0m                 \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    305\u001b[0m     )\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    530\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             new_data = concatenate_managers(\n\u001b[0m\u001b[1;32m    533\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbm_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/concat.py\u001b[0m in \u001b[0;36mconcatenate_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0;31m#  we can use np.concatenate, which is more performant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m#  than concat_compat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# TODO(EA2D): special-casing not needed with 2D EAs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "\n",
    "# Code from Canvas:\n",
    "#load pandas, tool for data analysis in Python\n",
    "import pandas as pd\n",
    "import langdetect\n",
    "# load nltk to detect stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "replace_dict = {'OM TJÄNSTEN': ' ', 'ARBETSUPPGIFTER': ' ', 'VI SÖKER DIG SOM': ' ', \n",
    "                 'INFORMATION OM FÖRETAGET': ' '} \n",
    "\n",
    "\n",
    "# read the dataset that is in the same directory and call it jobtech_dataset\n",
    "#jobtech_dataset = pd.read_csv('jobtech2022columns2updated.csv')\n",
    "jobtech_dataset = pd.read_csv('jobtech2022columns2updated.csv')\n",
    "\n",
    "# Specify the desired data types for the columns\n",
    "data_types = {'description': str, 'occupation': str}\n",
    "\n",
    "# Convert the columns to the desired data types\n",
    "jobtech_dataset = jobtech_dataset.astype(data_types)\n",
    "\n",
    "# #show the variables names (columns) in the dataset\n",
    "print(jobtech_dataset.columns)\n",
    "#show the first 3 rows (job postings) in the dataset\n",
    "print(jobtech_dataset.head(3))\n",
    "\n",
    "#jobtech_dataset.info()\n",
    "dataframe = jobtech_dataset\n",
    "\n",
    "df = pd.DataFrame(columns=dataframe.columns)\n",
    "\n",
    "print('detecting swedish ads..')\n",
    "\n",
    "\"\"\"\n",
    "# iterate over the rows of the original dataframe\n",
    "for index, row in dataframe.iterrows():\n",
    "    # detect the language of the description column using the langdetect library\n",
    "    if detect(row['description']) == 'sv':\n",
    "        # if the language is Swedish, append the row to the new dataframe\n",
    "        df = df.append(row, ignore_index=True)\n",
    "        \"\"\"\n",
    "\n",
    "'''\n",
    "# iterate over each row in the DataFrame\n",
    "for index, row in jobtech_dataset.iterrows():\n",
    "    # check if the 'description' value is a string or can be converted to a string\n",
    "    if isinstance(row['description'], str):\n",
    "        # perform language detection\n",
    "        if detect(row['description']) == 'sv':\n",
    "            # add row to new dataset if Swedish\n",
    "            df = df.append(row, ignore_index=True)\n",
    "    else:\n",
    "        # handle cases where 'description' is not a string\n",
    "        # e.g., you can choose to skip or handle these rows differently\n",
    "        print(\"Skipped row:\", index)\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "for index, row in dataframe.iterrows():\n",
    "    description = row['description']\n",
    "    if description.strip():  # Skip if the description is empty or contains only whitespace\n",
    "        try:\n",
    "            lang = langdetect.detect(description)\n",
    "            if lang == 'sv':\n",
    "                # Add the Swedish text to the data frame\n",
    "                df = df.append(row)\n",
    "        except langdetect.lang_detect_exception.LangDetectException:\n",
    "            # Handle the case when language detection fails\n",
    "            pass\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# cleaning the column description\n",
    "print('cleaning description column..')\n",
    "\n",
    "df['description'] = df['description'].replace(replace_dict, regex=True)\n",
    "\n",
    "#Remove stop words from description column\n",
    "print('removing stopwords..')\n",
    "stop_words = set(stopwords.words('swedish'))\n",
    "df['description'] = df['description'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word.lower() not in stop_words]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(df.columns)\n",
    "#show the first 3 rows (job postings) in the dataset\n",
    "print(df.head(3))\n",
    "df.info()\n",
    "\n",
    "# loop through each unique occupation and find the corresponding description(s)\n",
    "print('finding unique occupations..')\n",
    "occupations = df['occupation'].unique()\n",
    "occupation_descriptions = []\n",
    "for occupation in occupations:\n",
    "    description = \"\"\n",
    "    for index, row in df.iterrows():\n",
    "        if row['occupation'] == occupation:\n",
    "            description += row['description'] + \" \"\n",
    "    occupation_descriptions.append(description.strip())\n",
    "\n",
    "# create a new dataframe with the occupations and descriptions\n",
    "new_df = pd.DataFrame({'occupation': occupations, 'description': occupation_descriptions})\n",
    "\n",
    "# print the new dataframe\n",
    "#print(new_df)'''\n",
    "\n",
    "# save the new dataframe to a CSV file\n",
    "df.to_csv('dataset2022UPDATEDsv.csv', index=False)\n",
    "\n",
    "print('new csv created. end of program.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# STEG 1\n",
    "\n",
    "# Loading the dataset\n",
    "\n",
    "#load pandas, tool for data analysis in Python\n",
    "import pandas as pd\n",
    "import langdetect\n",
    "\n",
    "\n",
    "# read the dataset that is in the same directory and call it jobtech_dataset\n",
    "jobtech_dataset = pd.read_csv('jobtech2022columns2updated.csv')\n",
    "\n",
    "# Specify the desired data types for the columns\n",
    "data_types = {'description': str, 'occupation': str}\n",
    "\n",
    "# Convert the columns to the desired data types\n",
    "jobtech_dataset = jobtech_dataset.astype(data_types)\n",
    "\n",
    "# #show the variables names (columns) in the dataset\n",
    "print(jobtech_dataset.columns)\n",
    "#show the first 3 rows (job postings) in the dataset\n",
    "print(jobtech_dataset.head(3))\n",
    "\n",
    "#jobtech_dataset.info()\n",
    "dataframe = jobtech_dataset\n",
    "\n",
    "df = pd.DataFrame(columns=dataframe.columns)\n",
    "\n",
    "print('detecting swedish ads..')\n",
    "\n",
    "for index, row in dataframe.iterrows():\n",
    "    description = row['description']\n",
    "    if description.strip():  # Skip if the description is empty or contains only whitespace\n",
    "        try:\n",
    "            lang = langdetect.detect(description)\n",
    "            if lang == 'sv':\n",
    "                # Add the Swedish text to the data frame\n",
    "                df = df.append(row)\n",
    "        except langdetect.lang_detect_exception.LangDetectException:\n",
    "            # Handle the case when language detection fails\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "# save the new dataframe to a CSV file\n",
    "df.to_csv('dataset2022UPDATEDsv2.csv', index=False)\n",
    "print('new csv created. end of program.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n",
      "Chunk 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/09/srb9_l2x3gncdpn0_zx0r2pm0000gn/T/ipykernel_2861/3250501543.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'sv'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                     \u001b[0;31m# Add the Swedish text to the DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mlangdetect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang_detect_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLangDetectException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0;31m# Handle the case when language detection fails\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[1;32m   8941\u001b[0m             \u001b[0mcombined_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_diff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8942\u001b[0m             other = (\n\u001b[0;32m-> 8943\u001b[0;31m                 \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8944\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8945\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#steg 1 chunkat\n",
    "\n",
    "import pandas as pd\n",
    "import langdetect\n",
    "\n",
    "# Load the dataset\n",
    "jobtech_dataset = pd.read_csv('jobtech2022columns2updated.csv')\n",
    "\n",
    "# Specify the desired data types for the columns\n",
    "data_types = {'description': str, 'occupation': str}\n",
    "\n",
    "# Convert the columns to the desired data types\n",
    "jobtech_dataset = jobtech_dataset.astype(data_types)\n",
    "\n",
    "# Create an empty DataFrame to store the Swedish descriptions\n",
    "df = pd.DataFrame(columns=jobtech_dataset.columns)\n",
    "\n",
    "# Define the chunk size\n",
    "chunk_size = 100\n",
    "\n",
    "# Process the dataset in chunks\n",
    "for i in range(0, len(jobtech_dataset), chunk_size):\n",
    "    # Get the current chunk of rows\n",
    "    chunk = jobtech_dataset.iloc[i:i+chunk_size]\n",
    "    n = 1\n",
    "\n",
    "    # Detect Swedish ads in the current chunk\n",
    "    for index, row in chunk.iterrows():\n",
    "        description = row['description']\n",
    "        if description.strip():\n",
    "            try:\n",
    "                lang = langdetect.detect(description)\n",
    "                if lang == 'sv':\n",
    "                    # Add the Swedish text to the DataFrame\n",
    "                    df = df.append(row)\n",
    "            except langdetect.lang_detect_exception.LangDetectException:\n",
    "                # Handle the case when language detection fails\n",
    "                pass\n",
    "\n",
    "    # Save the DataFrame to the CSV file\n",
    "    df.to_csv('dataset2022UPDATEDsv2.csv', mode='a', index=False, header=(i == 0))\n",
    "    n += 1\n",
    "    print('Chunk', n)\n",
    "    \n",
    "\n",
    "\n",
    "    # Clear the DataFrame for the next chunk\n",
    "    df = pd.DataFrame(columns=jobtech_dataset.columns)\n",
    "\n",
    "print('New CSV created. End of program.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEG 2\n",
    "\n",
    "#Loading the dataset\n",
    "\n",
    "#load pandas, tool for data analysis in Python\n",
    "import pandas as pd\n",
    "# load nltk to detect stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "replace_dict = {'OM TJÄNSTEN': ' ', 'ARBETSUPPGIFTER': ' ', 'VI SÖKER DIG SOM': ' ', \n",
    "                 'INFORMATION OM FÖRETAGET': ' '} \n",
    "\n",
    "\n",
    "# read the dataset that is in the same directory and call it jobtech_dataset\n",
    "jobtech_dataset = pd.read_csv('dataset2022UPDATEDsv2.csv')\n",
    "\n",
    "# Specify the desired data types for the columns\n",
    "#data_types = {'description': str, 'occupation': str}\n",
    "\n",
    "# Convert the columns to the desired data types\n",
    "jobtech_dataset = jobtech_dataset.astype(data_types)\n",
    "\n",
    "#show the variables names (columns) in the dataset\n",
    "print(jobtech_dataset.columns)\n",
    "#show the first 3 rows (job postings) in the dataset\n",
    "print(jobtech_dataset.head(3))\n",
    "\n",
    "#jobtech_dataset.info()\n",
    "dataframe = jobtech_dataset\n",
    "\n",
    "df = pd.DataFrame(columns=dataframe.columns)\n",
    "\n",
    "\n",
    "# cleaning the column description\n",
    "print('cleaning description column..')\n",
    "df['description'] = df['description'].replace(replace_dict, regex=True)\n",
    "\n",
    "#Remove stop words from description column\n",
    "print('removing stopwords..')\n",
    "stop_words = set(stopwords.words('swedish'))\n",
    "df['description'] = df['description'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word.lower() not in stop_words]))\n",
    "\n",
    "\n",
    "\n",
    "print(df.columns)\n",
    "#show the first 3 rows (job postings) in the dataset\n",
    "print(df.head(3))\n",
    "#df.info()\n",
    "\n",
    "# loop through each unique occupation and find the corresponding description(s)\n",
    "print('finding unique occupations..')\n",
    "occupations = df['occupation'].unique()\n",
    "occupation_descriptions = []\n",
    "for occupation in occupations:\n",
    "    description = \"\"\n",
    "    for index, row in df.iterrows():\n",
    "        if row['occupation'] == occupation:\n",
    "            description += row['description'] + \" \"\n",
    "    occupation_descriptions.append(description.strip())\n",
    "\n",
    "# create a new dataframe with the occupations and descriptions\n",
    "new_df = pd.DataFrame({'occupation': occupations, 'description': occupation_descriptions})\n",
    "\n",
    "# print the new dataframe\n",
    "#print(new_df)'''\n",
    "\n",
    "# save the new dataframe to a CSV file\n",
    "df.to_csv('dataset2022UPDATED.csv', index=False)\n",
    "print('new csv created. end of program.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEG 2 chunkat\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "replace_dict = {'OM TJÄNSTEN': ' ', 'ARBETSUPPGIFTER': ' ', 'VI SÖKER DIG SOM': ' ',\n",
    "                 'INFORMATION OM FÖRETAGET': ' '}\n",
    "# n is a counter\n",
    "n = 1\n",
    "\n",
    "# Read the dataset in chunks of 100 rows\n",
    "chunk_size = 100\n",
    "for jobtech_dataset in pd.read_csv('dataset2022UPDATEDsv.csv', chunksize=chunk_size):\n",
    "    # Cleaning the column description\n",
    "    jobtech_dataset['description'] = jobtech_dataset['description'].replace(replace_dict, regex=True)\n",
    "\n",
    "    # Remove stop words from description column\n",
    "    stop_words = set(stopwords.words('swedish'))\n",
    "    jobtech_dataset['description'] = jobtech_dataset['description'].apply(\n",
    "        lambda x: ' '.join([word for word in word_tokenize(x) if word.lower() not in stop_words]))\n",
    "\n",
    "    # Loop through each unique occupation and find the corresponding description(s)\n",
    "    occupations = jobtech_dataset['occupation'].unique()\n",
    "    occupation_descriptions = []\n",
    "    for occupation in occupations:\n",
    "        description = \"\"\n",
    "        for index, row in jobtech_dataset.iterrows():\n",
    "            if row['occupation'] == occupation:\n",
    "                description += row['description'] + \" \"\n",
    "        occupation_descriptions.append(description.strip())\n",
    "\n",
    "    # Create a new dataframe with the occupations and descriptions\n",
    "    new_df = pd.DataFrame({'occupation': occupations, 'description': occupation_descriptions})\n",
    "\n",
    "    # Append the new dataframe to an existing CSV file\n",
    "    with open('dataset2022UPDATED.csv', 'a') as f:\n",
    "        new_df.to_csv(f, header=f.tell() == 0, index=False)\n",
    "        print('chunk', n)\n",
    "        n += 1\n",
    "\n",
    "print('New CSV created. End of program.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#steg 1.5\n",
    "# ta bort stopwords\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "replace_dict = {'OM TJÄNSTEN': ' ', 'ARBETSUPPGIFTER': ' ', 'VI SÖKER DIG SOM': ' ',\n",
    "                 'INFORMATION OM FÖRETAGET': ' '}\n",
    "\n",
    "# Read the dataset\n",
    "jobtech_dataset = pd.read_csv('dataset2022UPDATEDsv2.csv')\n",
    "\n",
    "# Cleaning the column description\n",
    "print('replacing titles..')\n",
    "jobtech_dataset['description'] = jobtech_dataset['description'].replace(replace_dict, regex=True)\n",
    "\n",
    "# Remove stop words from description column\n",
    "print('removing stopwords..')\n",
    "stop_words = set(stopwords.words('swedish'))\n",
    "jobtech_dataset['description'] = jobtech_dataset['description'].apply(\n",
    "    lambda x: ' '.join([word for word in word_tokenize(x) if word.lower() not in stop_words]))\n",
    "\n",
    "\n",
    "\n",
    "jobtech_dataset.to_csv('dataset2022UPDATEDstop.csv', index=False)\n",
    "print('new csv created. end of program.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#steg 2\n",
    "# Create unique occupations\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "#create a counter\n",
    "n = 1\n",
    "\n",
    "# Read the dataset\n",
    "jobtech_dataset = pd.read_csv('dataset2022UPDATEDstop.csv')\n",
    "\n",
    "\n",
    "# Loop through each unique occupation and find the corresponding description(s)\n",
    "occupations = jobtech_dataset['occupation'].unique()\n",
    "occupation_descriptions = []\n",
    "for occupation in occupations:\n",
    "    description = \"\"\n",
    "    for index, row in jobtech_dataset.iterrows():\n",
    "        if row['occupation'] == occupation:\n",
    "            description += row['description'] + \" \"\n",
    "            n += 1\n",
    "            print(n)\n",
    "    occupation_descriptions.append(description.strip())\n",
    "\n",
    "# Create a new dataframe with the occupations and descriptions\n",
    "new_df = pd.DataFrame({'occupation': occupations, 'description': occupation_descriptions})\n",
    "\n",
    "\n",
    "new_df.to_csv('dataset2022UPDATED.csv', index=False)\n",
    "print('new csv created. end of program.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

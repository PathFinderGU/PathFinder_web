{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ANVÄNDS EJ LÄNGRE\n",
    "\n",
    "## NLTK SET UP ##\n",
    "\n",
    "## Natural language processing, pandas setup\n",
    "\n",
    "#Import tokenization, download stopwords & punkt. Download not needed each run but left in for compatibility.\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import download \n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "## Feed variable with NLTK stopwords\n",
    "stop_words = set(stopwords.words(\"swedish\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## SET UP PANDAS & READ FILE ##\n",
    "import pandas as pd\n",
    "# read the file called 2022.json that is in the same directory and call it jobtech_dataset\n",
    "jobtech_dataset = pd.read_csv('jobtech_dataset2022.csv')\n",
    "#pd.set_option('max_colwidth', None) # Tell editor to not limit column width\n",
    "\n",
    "\n",
    "df = jobtech_dataset[['id', 'description', 'working_hours_type', 'must_have', 'nice_to_have']]  # Picking our columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choosing data & tokenizes it\n",
    "\n",
    "#min_variabel = df.iloc[0:100]['description']            # Taking the second row from the data and only 'description' column\n",
    "min_variabel = df['description']\n",
    "# Prints the entire description of a certain row for debug purposes\n",
    "print(min_variabel)\n",
    "\n",
    "# Tokenizes by word, each word becomes an entry in an array. Feed them into another variable. We need to fix our variable names.\n",
    "variabel = word_tokenize(str(min_variabel))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STOPWORDS & PUNCTUATION## + BLACKLIST\n",
    "\n",
    "\n",
    "\n",
    "#For each word in our variable, remove those that are stopwords\n",
    "filtrerad_lista = []\n",
    "for word in variabel:\n",
    "    if word.casefold() not in stop_words:\n",
    "        filtrerad_lista.append(word)\n",
    "\n",
    "\n",
    "# Ta bort punkter och sånt\n",
    "blacklist = [\"'\", \".\", \":\", \"!\", \"*\", \"None\", \",\"]\n",
    "ny_filtrerad_lista = []\n",
    "for word in filtrerad_lista:\n",
    "    if word not in blacklist:\n",
    "        ny_filtrerad_lista.append(word)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Prints our list of remaining, non-stop words\n",
    "\n",
    "print(ny_filtrerad_lista)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##STEMMING \n",
    "##Reducerar ord som helper och helping till help\n",
    "#Kommentar: Funkar inte görbra på svenska - precis som Vasili sa\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "stemmed_lista = [stemmer.stem(word) for word in ny_filtrerad_lista]\n",
    "\n",
    "print(stemmed_lista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lemm (konverterar de stemmade orden till ord som betyder något)\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "ny_lista = []\n",
    "for word in stemmed_lista: \n",
    "    ny_lista.append(lemmatizer.lemmatize(word))\n",
    "\n",
    "print(ny_lista)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magin\n",
    "#nltk.download(\"book\")\n",
    "from nltk import FreqDist\n",
    "\n",
    "frequency_distribution = FreqDist(ny_lista)\n",
    "\n",
    "print(frequency_distribution)\n",
    "ny_variabel = frequency_distribution.most_common(50)\n",
    "\n",
    "print(ny_variabel)\n",
    "\n",
    "ny_variabel = frequency_distribution.most_common(20)\n",
    "print(\"\\nTop 50 most common words:\")\n",
    "for word, freq in ny_variabel:\n",
    "    print(f\"{word}: {freq}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOT\n",
    "\n",
    "frequency_distribution.plot(20, cumulative=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
